[["index.html", "データハンドリング実践入門 - RとSQLの比較 データハンドリング実践入門 - RとSQLの比較 目的 構成 制約", " データハンドリング実践入門 - RとSQLの比較 前田和寛(@kazutan) 2021-09-12 データハンドリング実践入門 - RとSQLの比較 目的 分析業務の現場で行うデータハンドリング事例を紹介 同一の加工をRとSQLで実践する SQL入門も兼ねて 構成 (WIP) 制約 ここでは基本的にテーブル構造のデータを中心に説明していきます。 list/arrayタイプのようなデータ構造はここでは詳細に触れません。 ご了承ください。 "],["01_r_and_sql_setting.html", "1 データハンドリングの考え方/環境準備 1.1 構成 1.2 データハンドリングの考え方 1.3 Rによるデータハンドリング 1.4 SQLによるデータハンドリング 1.5 参考資料", " 1 データハンドリングの考え方/環境準備 1.1 構成 データハンドリングの考え方 基本的な流れ Rによるデータハンドリング イメージ 環境準備 SQLによるデータハンドリング イメージ 環境準備 1.2 データハンドリングの考え方 ぶっちゃけ、料理と一緒ですね。 1.2.1 基本的な流れ データハンドリングは、基本的に以下の流れを1つのunitとして考えます: データオブジェクトを準備 削除 カラム(列)の選択/削除(横を変える) レコード(行)の選択/削除(縦を変える) 加工/追加 カラム内の値/型の変換 1 カラムを追加 集約 集約化変数の指定 集約による行圧縮 結合 結合対象のデータオブジェクトを準備 結合方向の指定 データオブジェクト間の結合 データオブジェクトの出力 そして、あとはこれをつなげていって全体を達成します1。｢えっ、group_byして集約した後に加工したりするじゃん?｣と思うかもしれません。しかしそれは、以下のように考えることができます: unit 1 df_A を準備 (削りなしなのでスキップ) (加工なしなのでスキップ) 集約 group_by して集計値を出す (結合なしなのでスキップ) df_A_01 を出力 unit 2 df_A_01を準備 (削りなしなのでスキップ) 加工 sumがmeanを超えたら1とする etc… (集約なしなのでスキップ) (結合なしなのでスキップ) df_A_02を出力 これでスキップをなくすと、以下のようになります: unit 1 df_A を準備 集約 group_by して集計値を出す df_A_01 を出力 unit 2 df_A_01を準備 加工 sumがmeanを超えたら1とする etc… df_A_02を出力 さらに、unit_1の出力とunit_2の準備(入力)は繋がっているので、こういう書き方(太字)もできます: unit 1 df_A を準備 集約 group_by して集計値を出す df_A_01 を出力 unit 2 unit 1の出力df_A_01を準備 加工 sumがmeanを超えたら1とする etc… df_A_02を出力 これにより、2つのunitが繋がった書き方となります。これで依存関係なども表現できるようになりましたね。 データハンドリングは、 最終的に欲しい形のデータを決める 最終形態に必要なデータを準備 unitおよびフローを｢計画｣する 計画したunit flowを実行できるよう｢記述｣する 記述した計画を実行する …となります。データハンドリングをする際は、必ずこれを意識してください。 1.3 Rによるデータハンドリング 今なら宇宙船本がありますよ! https://gihyo.jp/book/2021/978-4-297-12170-9 1.3.1 イメージ 今回はtidyverseの世界で説明します。 data.frameおよびtibbleは、ともに列志向のデータオブジェクトです。｢列志向｣とは｢データの区切りとしてまず列がある｣というものです。｢dfは7つのcolumnがある｣-&gt;｢1つの列は100のデータを持つ｣…という階層構造になっていることを指します2。従って、列を意識する必要があります。 Rでもデータハンドリングの考え方は上述のとおりで、tidyverseはunitの各機能に対応するfunctionを提供しています。また、パイプ演算子はunit間を繋いでいく役目をしていますので、こんな感じの表現もできますね: unit 1 &amp; unit 2 df_A を準備 集約 group_by して集計値を出す (出力が生成されるけどpipeされる) %&gt;% ( %&gt;% で送られてきたものを受け取る) 加工 sumがmeanを超えたら1とする etc… df_A_02を出力 あとはこれをRのスクリプトで｢記述｣すればOKですね。 1.3.2 環境準備 今回はtidyverseの世界で説明します。 R/RStudioをインストール tidyverseを入れる 1.4 SQLによるデータハンドリング 1.4.1 SQL/RDBの基礎 このスライド以上の説明がないのでこれみてください: この内容をある程度網羅的に実践して、いくつか実際に書いてみればだいたい書けるようになります。なので基礎部分は省略。 1.4.2 イメージ SQLは、｢データを加工するプロセスを記述するための言語｣です。なので、上述したunitのプロセスを書き起こしたものとなります。Rではわりとフローを意識したコーディングになるのですが、SQLではunitを非常に意識したコーディングとなります。 …というか、このunitという観点については、私はSQLでデータハンドリングしててそう考えるようになりました。なので、上述のunitのイメージができるようになれば、あとは辞書的なものを参照しながら書けるようになります。 1.4.3 環境準備 今回はRDBとしてSQLiteを利用します。SQLiteは以下の特徴があります: ファイルをDB dataとして扱える 軽量で手元で簡単にできる そこまで大きくないデータであれば十分に使える 独自関数はあんまりない(逆に入門として扱いやすい) でもwindow関数は使える https://www.sqlite.org/windowfunctions.html もちろんR/RStudioから利用可能 (ほかは口頭で) 1.4.3.1 SQLiteの準備 Windowsの場合 適当にググってください Macの場合 デフォルトでインストールされています Ubuntuの場合 sudo apt install sqlite でOK Terminalを起動して、sqlite3を実行して出てきてくれればOK。終了コマンドは .quitです。ヘルプは.helpで表示されます。 1.5 参考資料 (WIP) ピボット変換(wideとlongの変換)がここには入っていません。これはSQLでのピボット変換コードを見るとわかるのですが、実は｢削除｣｢加工｣｢集約｣｢結合｣を組み合わせて表現できます。また並べ替えについても同様です。↩︎ よく誤解されますが、Rにおいて行列は｢ベクトル｣です。一見data.frameっぽく見えますが、あれはベクトルに指定した次元情報をもたせただけの｢1つのベクトル｣です。なので全ての値で同一のデータ型しか許可しません。列志向も行志向もないです。↩︎ "],["02_prepare_exec_env.html", "2 実行環境の準備 2.1 libraryの準備 2.2 RSQLite チュートリアル 2.3 今回のデータの準備", " 2 実行環境の準備 2.1 libraryの準備 個人的にRStudio上で全て作業したいので、それを前提とします。 2.1.1 R/RStudio, SQLiteのインストール -&gt; 前節参照 2.1.2 RStudioからSQLiteを扱えるようにする RSQLiteパッケージ RからSQLiteへアクセスするためのドライバなどを提供するパッケージ 開発陣が大物ばかり DBIパッケージ RからDBへ接続する関数を提供するパッケージ RSQLiteパッケージをインストールすれば多分入るはず 2.2 RSQLite チュートリアル これを読もう https://rsqlite.r-dbi.org/articles/rsqlite これを簡単に試しましょう 2.2.1 接続テスト …その前に、｢開いたconは閉じる｣を頭に入れておいてください。 # library library(DBI) library(tidyverse) library(lubridate) # connect DB # 指定したファイルパスへsqliteで接続(connect) # 指定したパスにファイルがない場合、下のファイル名で作成される mydb &lt;- dbConnect(RSQLite::SQLite(), &quot;my-db.sqlite&quot;) # 接続を切断 dbDisconnect(mydb) # 接続先を削除 # SQLiteで上記のようにディスク上のファイルを指定した場合、｢そのファイルが削除｣される unlink(&quot;my-db.sqlite&quot;) # mydbは｢接続情報｣オブジェクト # なので、これを使って再度connectできる # さっきunlinkしたけど、これでまた復帰します mydb &lt;- dbConnect(mydb) # dbConnectを書いたらすぐにdbDisconnectを書きましょう dbDisconnect(mydb) 2.2.2 tableを作る とりあえず、Rのいくつかのdata.frameをDB内のtableに書き出してみましょう # write table and check----- # dbへconnect mydb_writetbl &lt;- dbConnect(RSQLite::SQLite(), # DB接続のドライバを指定 &quot;my-db.sqlite&quot;) # 接続先を指定(今回のSQLiteはファイルパス) # write table - iris dbWriteTable(mydb_writetbl, # dbConnectオブジェクト、すなわち接続するDB &quot;t_iris&quot;, # table名 iris, # 書き込むデータ。ここではiris overwrite = TRUE) # 上書きするか。実際には細心の注意を # write table - mtcars dbWriteTable(mydb_writetbl, &quot;t_mtcars&quot;, mtcars, overwrite = TRUE) # 細心の注意を # 閉じる dbDisconnect(mydb_writetbl) # 開いたconは閉じろ これで、2つのtableに書き出せたので、実際に入っているか確認しましょう # もう一回DBにつなぐ # connectionオブジェクトだけを指定してもOK con &lt;- dbConnect(mydb) # DB内の全てのtableをリストアップ dbListTables(con) ## [1] &quot;t_iris&quot; &quot;t_mtcars&quot; これで2つのテーブルが書けたので、簡単にテストをしていきましょう。 2.2.3 tableへの問い合わせ(query) queryとは｢問い合わせる｣という意味です。なのでここでは｢DBにぶん投げる問い合わせ内容｣となります。 # irisを5行もってくる dbGetQuery(con, #接続先 &#39;select * from t_iris limit 5&#39;) # SQL query ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa もちろんsqlの結果をR側のオブジェクトとして受けることもできます iris_from_db &lt;- dbGetQuery(con, &#39;select * from t_mtcars&#39;) head(iris_from_db, 3) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## 2 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## 3 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 現在、パラメータ化も使えるようになってます # 詳細はチュートリアルを参照 dbGetQuery(con, &#39;SELECT * FROM t_iris WHERE &quot;Sepal.Length&quot; &lt; :x&#39;, params = list(x = 4.6)) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 4.4 2.9 1.4 0.2 setosa ## 2 4.3 3.0 1.1 0.1 setosa ## 3 4.4 3.0 1.3 0.2 setosa ## 4 4.5 2.3 1.3 0.3 setosa ## 5 4.4 3.2 1.3 0.2 setosa # 開いたconは閉じろ dbDisconnect(con) 2.3 今回のデータの準備 今回は、以下のsiteの仮想データを利用します: ログデータ処理で始めるlubridate入門 ここの2に、仮想ログデータを作成するRコードがあります ただ、今回はDB的なデータ構造にしたいので、DBの正規化をしてitem masterを分離させます。正規化については適当にググってください。 2.3.1 R側の仮想データ作成 上記のサイトのコードをベースに作成します: # params setting start &lt;- &quot;2018-1-1 00:00:00&quot; #開始日 n &lt;- 10000 # 購入件数 duration_days &lt;- 50 # ログの期間(日数) list_price &lt;- c(100, 500, 1000, 2000, 5000) # アイテムの価格リスト list_item &lt;- paste(&quot;item&quot;, 1:length(list_price), sep = &quot;_&quot;) # アイテムリスト list_item_p &lt;- c(100, 50, 10, 5, 2) # 発生比 list_id &lt;- 1000001:1000300 # 会員id # ログデータ生成 df_log &lt;- data.frame( # タイムスタンプを作成 # 開始日時を生成 stamp = ymd_hms(start) + # 0-50までの整数からランダムに10000件生成し、それを日数データに変換して足す days(sample(0:duration_days, n, replace = TRUE)) + # 0-23までの整数からランダムに10000件生成し、それを時間データに変換して足す hours(sample(0:23, n, replace = TRUE)) + # 0-59までの整数からランダムに10000件生成し、それを分データに変換して足す minutes(sample(0:59, n, replace = TRUE)) + # 0-59までの整数からランダムに10000件生成し、それを病データに変換して足す seconds(sample(0:59, n, replace = TRUE)), # 会員IDをランダムに生成 id = sample(list_id, n, replace = TRUE), # アイテム名をランダムに生成 item = sample(list_item, n, replace = TRUE, prob = list_item_p) ) %&gt;% # ログデータっぽく、タイムスタンプで並べ替える arrange(stamp) # 生成できているかを確認 knitr::kable(sample_n(df_log, 10)) stamp id item 2018-01-18 17:44:00 1000145 item_1 2018-01-08 21:58:20 1000090 item_1 2018-01-02 23:53:14 1000147 item_1 2018-01-04 07:53:51 1000166 item_1 2018-01-23 23:32:07 1000020 item_1 2018-01-30 00:13:59 1000069 item_1 2018-01-07 16:25:38 1000295 item_2 2018-01-19 10:03:55 1000209 item_1 2018-01-26 05:32:48 1000244 item_1 2018-02-05 06:29:16 1000204 item_1 今回、時間が大きな数値になってますが、これは仕様的にunixtimeになっているためです。あわせて、itemのmaster tableを作成します # itemのmaster dfを作成 df_item_master &lt;- tibble( item = list_item, value = list_price ) # 生成できているかを確認 knitr::kable(df_item_master) item value item_1 100 item_2 500 item_3 1000 item_4 2000 item_5 5000 2.3.2 DB側の準備 作成したRのオブジェクトをtableに書き出します # conを開く con &lt;- dbConnect(con) # tbl書き込み dbWriteTable(con, &quot;t_df_log&quot;, df_log, overwrite = TRUE) #今日は何度も書くので dbWriteTable(con, &quot;t_df_item_master&quot;, df_item_master, overwrite = TRUE) #今日は何度も書くので # 生成できているかをチェック dbGetQuery(con, &#39;select * from t_df_log limit 5&#39;) ## stamp id item ## 1 1514765056 1000236 item_2 ## 2 1514765209 1000199 item_2 ## 3 1514765540 1000121 item_1 ## 4 1514766089 1000240 item_2 ## 5 1514766165 1000300 item_2 dbGetQuery(con, &#39;select * from t_df_item_master&#39;) ## item value ## 1 item_1 100 ## 2 item_2 500 ## 3 item_3 1000 ## 4 item_4 2000 ## 5 item_5 5000 これでDBに作成できたので、実際にRを比較していきましょう。開いたconは閉じましょう。 # 開いたconは閉じろ dbDisconnect(con) "],["03_logdata_handling_r_and_sql.html", "3 ログデータハンドリング - RとSQLでの比較 3.1 データの準備 3.2 Q1 日別の売上集計", " 3 ログデータハンドリング - RとSQLでの比較 3.1 データの準備 前節で作成したデータはDBにあります。てことでR側はそこから取得しましょう。 # library library(DBI) library(tidyverse) library(lubridate) # db con # 事前に前節でデータを作成している前提です con &lt;-dbConnect(RSQLite::SQLite(), &quot;my-db.sqlite&quot;) # read tableして格納 df_log &lt;- dbReadTable(con, &quot;t_df_log&quot;) df_item_master &lt;- dbReadTable(con, &quot;t_df_item_master&quot;) SQLは直接queryを投げるので、ここでは特に必要ありません。 3.2 Q1 日別の売上集計 日別の売上合計を算出しましょう 3.2.1 Rによるデータハンドリング df_res_01_r &lt;- df_log %&gt;% left_join(df_item_master) %&gt;% # item masterの情報でvalueを持ってくる mutate(date_time = as_datetime(stamp, tz = &#39;Asia/Tokyo&#39;), # なぜかunixtimeになってた… date = date(date_time)) %&gt;% # timestampからdateに変換 group_by(date) %&gt;% # 日付でgroup_by summarise( n = n(), total = sum(value) ) # チェック head(df_res_01_r, 5) ## # A tibble: 5 x 3 ## date n total ## &lt;date&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2018-01-01 130 53800 ## 2 2018-01-02 199 74400 ## 3 2018-01-03 209 80800 ## 4 2018-01-04 169 60200 ## 5 2018-01-05 214 69500 unitに分けて考えます unit_1: logに価格を追加 入力: df_logを準備 (削除はスキップ) (加工はスキップ) (集約はスキップ) 結合: df_item_masterを準備 left join keyになる変数は両方とも item 出力: %&gt;% unit_2へ unit_2 日付のカラムを作成 入力: unit_1からpipe (削除はスキップ) 加工 stampがunixtimeなのでdate-time型 date_timeへ date-time型 date_time から date型 date へ (集約はスキップ) (結合はスキップ) 出力: %&gt;% unit_3へ unit_3: 集約して集計 入力: unit_2からpipe 削除: date以外を除外 加工 count -&gt; nへ sum -&gt; totalへ 集約: dateでgroup_by (結合はスキップ) 出力: df_res_01_rとして出力 このような感じになります。 3.2.2 SQLによるデータハンドリング では、SQLによって同じ操作を行います # sql_queryを作成 query_01 &lt;- &quot; select date ,count(*) as n ,sum(value) as total from ( select unit_1.* ,date(datetime(stamp, &#39;unixepoch&#39;, &#39;localtime&#39;)) as date from ( select log.* ,im.value from t_df_log as log left join t_df_item_master as im on log.item = im.item ) as unit_1 ) as unit_2 group by 1 order by 1 &quot; df_res_01_sql &lt;- dbGetQuery(con, query_01) head(df_res_01_sql, 5) ## date n total ## 1 2018-01-01 130 53800 ## 2 2018-01-02 199 74400 ## 3 2018-01-03 209 80800 ## 4 2018-01-04 169 60200 ## 5 2018-01-05 214 69500 同一の結果が得られました。 さて、上記のSQLコードはRで書いたunitでの考え方に合わせてます。 unit_1: logに価格を追加 入力: t_df_logを準備 (削除はスキップ) (加工はスキップ) (集約はスキップ) 結合: t_df_item_masterを準備 left join keyになる変数は両方とも item 出力: unit_1 unit_2 日付のカラムを作成 入力: unit_1 (削除はスキップ) 加工 stampがunixtimeなのでdate-time型 date_timeへ date-time型 date_time から date型 date へ (集約はスキップ) (結合はスキップ) 出力: unit_2 unit_3: 集約して集計 入力: unit_2 削除: date以外を除外 加工 count -&gt; nへ sum -&gt; totalへ 集約: dateでgroup_by (結合はスキップ) 出力: df_res_01_sqlとして出力 実際の処理としてはこの流れになります。あとは適宜実際のクエリと照らし合わせてください。 なお個人的にはRでsqlを書くときは別ファイルにして*.sqlとして、RStudio上で作業します。 # 開いたconは閉じろ dbDisconnect(con) "],["04_pivot_handling_r_and_sql.html", "4 pivotハンドリング - RとSQLでの比較 4.1 データの準備 4.2 from long to wide 4.3 from wide to long", " 4 pivotハンドリング - RとSQLでの比較 ここでは、long-wide変換であるpivotを取り上げます。pivot(long-wide)変換自体の説明については省略します。 4.1 データの準備 いわゆるlong型になっているデータとしては、dbにはt_df_logがlong型になってます。なのでこれを今回は利用します。 wide型になっているデータとしては、dbには書き込みテストで利用した t_irisがあります。 これを利用してみましょう。 4.2 from long to wide long型からwide型へ変換します。t_df-logで、item別個数を横へ広げます。レコードはユーザー単位です。つまり、｢ユーザーごとに各アイテムを何個ずつ購入したか｣を示す表になります。 4.2.1 expand to wide by SQL 先にSQLで記述します。 別ファイルでsqlを書いて、それを呼び出すこととします。 # library library(tidyverse) library(DBI) # 先にsqlファイルを読み込む関数を準備 read_sql &lt;- function(file) { string &lt;- readLines(file) res &lt;- paste(string, collapse = &quot;\\n&quot;) return(res) } # queryを格納 query_02_wider &lt;- read_sql(&quot;sql/pivot_wide.sql&quot;) # queryを表示 cat(query_02_wider) ## select ## id ## ,sum(case when item = &#39;item_1&#39; then 1 end) as item_1 ## ,sum(case when item = &#39;item_2&#39; then 1 end) as item_2 ## ,sum(case when item = &#39;item_3&#39; then 1 end) as item_3 ## ,sum(case when item = &#39;item_4&#39; then 1 end) as item_4 ## ,sum(case when item = &#39;item_5&#39; then 1 end) as item_5 ## from ## t_df_log ## group by ## id # 問い合わせる con &lt;- dbConnect(con) df_res_02_sql &lt;- dbGetQuery(con, query_02_wider) head(df_res_02_sql) ## id item_1 item_2 item_3 item_4 item_5 ## 1 1000001 19 12 2 NA NA ## 2 1000002 20 14 1 2 NA ## 3 1000003 25 6 1 1 NA ## 4 1000004 23 13 3 NA NA ## 5 1000005 20 8 5 1 NA ## 6 1000006 19 22 2 1 NA SQLiteでwideに展開しようとするなら、基本的には上記のようなクエリになります。unitで考えると以下のようになります: unit_1: pivot_wide 入力: t_df_logを準備 削除: 列選択: idとitemだけを取り出す (行選択はスキップ) 加工: item -&gt; item_1 変換: item = item_1なら1、そうでないなら0 集約: これをsum item -&gt; item_2 変換: item = item_2なら1、そうでないなら0 合計: これをsum item -&gt; item_3 変換: item = item_3なら1、そうでないなら0 合計: これをsum item -&gt; item_4 変換: item = item_4なら1、そうでないなら0 合計: これをsum item -&gt; item_5 変換: item = item_5なら1、そうでないなら0 合計: これをsum 集約: 集約化変数: id (結合はスキップ) 今回のsqlではunit_1だけになりましたが、他の書き方もあります: # queryを格納 query_02_wider2 &lt;- read_sql(&quot;sql/pivot_wide2.sql&quot;) # queryを表示 cat(query_02_wider2) ## select ## id ## ,max(case when item = &#39;item_1&#39; then cnt end) as item_1 ## ,max(case when item = &#39;item_2&#39; then cnt end) as item_2 ## ,max(case when item = &#39;item_3&#39; then cnt end) as item_3 ## ,max(case when item = &#39;item_4&#39; then cnt end) as item_4 ## ,max(case when item = &#39;item_5&#39; then cnt end) as item_5 ## from ( ## select ## id ## ,item ## ,count(*) as cnt ## from ## t_df_log ## group by ## id ## ,item ## ) unit_1 ## group by ## id # 問い合わせる con &lt;- dbConnect(con) df_res_02_sql2 &lt;- dbGetQuery(con, query_02_wider2) head(df_res_02_sql2) ## id item_1 item_2 item_3 item_4 item_5 ## 1 1000001 19 12 2 NA NA ## 2 1000002 20 14 1 2 NA ## 3 1000003 25 6 1 1 NA ## 4 1000004 23 13 3 NA NA ## 5 1000005 20 8 5 1 NA ## 6 1000006 19 22 2 1 NA 今回はこのような処理になります: unit 1: id, itemでcount集計 入力: t_df_log 削除: id, itemを選択 加工: count: -&gt; cnt 集約: 集約化変数: id, item (結合はないのでスキップ) 出力: unit_1 unit 2: pivot wide 入力: unit_1 削除: 列選択: idを取り出す 加工: item -&gt; item_1 変換: item = item_1ならcnt、そうでないならnull 集約: これをmax item -&gt; item_2 変換: item = item_2ならcnt、そうでないならnull 集約: これをmax item -&gt; item_3 変換: item = item_3ならcnt、そうでないならnull 集約: これをmax item -&gt; item_4 変換: item = item_4ならcnt、そうでないならnull 集約: これをmax item -&gt; item_5 変換: item = item_5ならcnt、そうでないならnull 集約: これをmax 集約: 集約化変数: id (結合はスキップ) unit 2の加工でmaxを使っているけど、これは対象レコードが1しかないはずなので、 別にmaxでもminでもどれでも同値になります(でもなぜかmaxを利用する例が多い)。 最初の例は、この2つめの例をショートカットしたものとなります。集約ロジックがただのcountなのであんまり変わらないですが、複雑な計算をさせる場合は効率性などは意識したほうがいいでしょう。 4.2.2 expand to wide by R tidyr::pivot_wider使ってください。ちなみにSQLのところで記述してるアプローチ2つをRでもかき分けることができます。ぜひチャレンジしてみてください。 そして、書けた後でSQLのクエリ/ロジックと比較してみてください。pivotがどのような処理をしているかが見えてくると思います。 4.3 from wide to long t_irisを使ってやってみましょう 4.3.1 expand to long by SQL 正直めんどくさいのねん…SQLは別ファイルで書きました。 # queryを格納 query_02_long &lt;- read_sql(&quot;sql/pivot_long.sql&quot;) # queryを表示 cat(query_02_long) ## select ## &#39;Sepal.Length&#39; as var_name ## ,`Sepal.Length` as value ## , Species ## from ## t_iris ## union all ## select ## &#39;Sepal.Width&#39; as var_name ## ,`Sepal.Width` as value ## , Species ## from ## t_iris ## union all ## select ## &#39;Petal.Length&#39; as var_name ## ,`Petal.Length` as value ## , Species ## from ## t_iris ## union all ## select ## &#39;Petal.Width&#39; as var_name ## ,`Petal.Width` as value ## , Species ## from ## t_iris # 問い合わせる con &lt;- dbConnect(con) df_res_02_sql3 &lt;- dbGetQuery(con, query_02_long) sample_n(df_res_02_sql3, 10) ## var_name value Species ## 1 Petal.Length 5.1 virginica ## 2 Petal.Width 0.1 setosa ## 3 Sepal.Width 3.2 versicolor ## 4 Petal.Length 1.5 setosa ## 5 Petal.Width 0.1 setosa ## 6 Sepal.Width 2.6 versicolor ## 7 Sepal.Length 6.0 virginica ## 8 Sepal.Width 3.0 setosa ## 9 Sepal.Length 6.2 versicolor ## 10 Sepal.Length 5.6 versicolor unit記述、ぜひやってみてください! # 開いたconは閉じよう dbDisconnect(con) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
